{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<table align=\"center\">\n  <td align=\"center\"><a target=\"_blank\" href=\"http://introtodeeplearning.com\">\n        <img src=\"https://i.ibb.co/Jr88sn2/mit.png\" style=\"padding-bottom:5px;\" />\n      Visit MIT Deep Learning</a></td>\n  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/MITDeepLearning/introtodeeplearning/blob/master/lab3/solutions/LLM_Finetuning_Solution.ipynb\">\n        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/MITDeepLearning/introtodeeplearning/blob/master/lab3/solutions/LLM_Finetuning_Solution.ipynb\">\n        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n</table>\n\n# Copyright Information","metadata":{"id":"yh8WeSsQfnyw"}},{"cell_type":"code","source":"# Copyright 2026 MIT Introduction to Deep Learning. All Rights Reserved.\n#\n# Licensed under the MIT License. You may not use this file except in compliance\n# with the License. Use and/or modification of this code outside of MIT Introduction\n# to Deep Learning must reference:\n#\n# Â© MIT Introduction to Deep Learning\n# http://introtodeeplearning.com\n#","metadata":{"id":"O-pRdpMbfnyw"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Laboratory 3: Large Language Model (LLM) Fine-tuning\n\nIn this lab, you will fine-tune a multi-billion parameter large language model (LLM). We will go through several fundamental concepts of LLMs, including tokenization, templates, and fine-tuning. This lab provides a complete pipeline for fine-tuning a language model to generate responses in a specific style, and you will explore not only language model fine-tuning, but also ways to evaluate the performance of a language model.\n\nYou will use [Liquid AI's](https://www.liquid.ai/) [LFM2-1.2B](https://huggingface.co/LiquidAI/LFM2-1.2B) as the base language model to fine-tune; Google's [Gemini 2.5](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/) model as an evaluation \"judge\" model; and Comet ML's [Opik](https://www.comet.com/site/products/opik/) as a framework for streamlined LLM evaluation.\n\nFirst, let's download the MIT deep learning package, install dependencies, and import the relevant packages we'll need for this lab.","metadata":{"id":"StmM5Grmfnyx"}},{"cell_type":"code","source":"# Install and import MIT Deep Learning utilities\n!pip install mitdeeplearning > /dev/null 2>&1\nimport mitdeeplearning as mdl","metadata":{"id":"fmkjWI4fVeAh","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:33:44.707257Z","iopub.execute_input":"2026-02-15T10:33:44.707919Z","iopub.status.idle":"2026-02-15T10:34:17.669854Z","shell.execute_reply.started":"2026-02-15T10:33:44.707890Z","shell.execute_reply":"2026-02-15T10:34:17.668990Z"}},"outputs":[{"name":"stderr","text":"2026-02-15 10:33:59.270475: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771151639.425858      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771151639.470847      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771151639.816705      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771151639.816740      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771151639.816743      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771151639.816745      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nGym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\nPlease upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\nSee the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  return datetime.utcnow().replace(tzinfo=utc)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model\nfrom lion_pytorch import Lion","metadata":{"id":"Oo64stjwBvnB","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:34:21.099391Z","iopub.execute_input":"2026-02-15T10:34:21.100008Z","iopub.status.idle":"2026-02-15T10:34:29.268816Z","shell.execute_reply.started":"2026-02-15T10:34:21.099977Z","shell.execute_reply":"2026-02-15T10:34:29.268028Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  return datetime.utcnow().replace(tzinfo=utc)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Part 1: Fine-tuning an LLM for style\n\nIn the first part of this lab, we will fine-tune an LLM as a chatbot that can generate responses in a specific style. We will use the [Liquid AI LFM2-1.2B model](https://huggingface.co/LiquidAI/LFM2-1.2B) as the base language model to finetune.","metadata":{"id":"j-qsDChnfnyx"}},{"cell_type":"markdown","source":"## 1.1: Templating and tokenization\n\n### 1.1.1: Templating\n\nLanguage models that function as chatbots are able to generate responses to user queries -- but how do they do this? We need to provide them with a way to understand the conversation and generate responses in a coherent manner -- some structure of what are inputs and outputs.\n\n[Templating](https://huggingface.co/docs/transformers/main/chat_templating) is a way to format inputs and outputs in a consistent structure that a language model can understand. It involves adding special tokens or markers to indicate different parts of the conversation, like who is speaking and where turns begin and end. This structure helps the model learn the proper format for generating responses and maintain a coherent conversation flow. Without templates, the model may not know how to properly format its outputs or distinguish between different speakers in a conversation.\n\nLet's start by defining some basic templates for the LFM2-based chatbot, for turns where the user asks a question and the model responds with an answer.","metadata":{"id":"VNE6ArjFfnyx"}},{"cell_type":"code","source":"# Basic question-answer template\ntemplate_without_answer = \"<|startoftext|><|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\ntemplate_with_answer = template_without_answer + \"{answer}<|im_end|>\\n\"\n\n# Let's try to put something into the template to see how it looks\nprint(template_with_answer.format(question=\"What is your name?\", answer=\"My name is Lili!\"))","metadata":{"id":"TN2zHVhfBvnE","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:34:34.183430Z","iopub.execute_input":"2026-02-15T10:34:34.184417Z","iopub.status.idle":"2026-02-15T10:34:34.188785Z","shell.execute_reply.started":"2026-02-15T10:34:34.184381Z","shell.execute_reply":"2026-02-15T10:34:34.188000Z"}},"outputs":[{"name":"stdout","text":"<|startoftext|><|im_start|>user\nWhat is your name?<|im_end|>\n<|im_start|>assistant\nMy name is Lili!<|im_end|>\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### 1.1.2: Tokenization\n\nTo operate on language, we need to prepare the text for the model. Fundamentally we can think of language as a sequence of \"chunks\" of text. We can split the text into individual chunks, and then map these chunks to numerical tokens -- collectively this is the process of [tokenization](https://huggingface.co/docs/transformers/main/tokenizer_summary). Numerical tokens can then be fed into a language model.\n\nThere are several common approaches to tokenizing natural language text:\n\n1. **Word-based tokenization**: splits text into individual words. While simple, this can lead to large vocabularies and does not handle unknown words well.\n\n2. **Character-based tokenization**: splits text into individual characters. While this involves a very small vocabulary, it produces long sequences and loses word-level meaning.\n\n3. **Subword tokenization**: breaks words into smaller units (subwords) based on their frequency. The most popular and commonly used approach is [byte-pair encoding (BPE)](https://en.wikipedia.org/wiki/Byte_pair_encoding), which iteratively merges the most frequent character pairs. Modern language models typically use subword tokenization as it balances vocabulary size and sequence length while handling unknown words effectively by breaking them into known subword units.\n\nIn this lab we will use the tokenizer from the LFM2 model, which uses BPE. Let's load it and inspect it.","metadata":{"id":"keh0rVN-fnyx"}},{"cell_type":"code","source":"# Load the tokenizer for Liquid AI LFM2-1.2B\nmodel_id = \"LiquidAI/LFM2-1.2B\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)  # use AutoTokenizer to load tokenizer from remote repo\n\n# How big is the tokenizer?\nprint(f\"Vocab size: {len(tokenizer.get_vocab())}\")","metadata":{"id":"EeDF1JI-BvnF","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:34:37.062929Z","iopub.execute_input":"2026-02-15T10:34:37.063603Z","iopub.status.idle":"2026-02-15T10:34:37.984033Z","shell.execute_reply.started":"2026-02-15T10:34:37.063578Z","shell.execute_reply":"2026-02-15T10:34:37.983240Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c4582307ed44440a3b90d9fcbeeeb20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f55c1a4e0f4348619dc239948119c7f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f11f539694c402d963057ecc4e31a05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef2474bebd684662ae0b3749d220548f"}},"metadata":{}},{"name":"stdout","text":"Vocab size: 64400\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"We not only need to be able to tokenize the text into tokens (encode), but also de-tokenize the tokens back into text (decode). Our tokenizer will have:\n1. an `encode` function to tokenize the text into tokens, and\n2. a `decode` function to de-tokenize back to text so that we can read out the model's outputs.\n\nLet's test out both steps and inspect to get a better understanding of how this works.","metadata":{"id":"sSJxCx6Nfnyx"}},{"cell_type":"code","source":"# Lets test out both steps:\ntext = \"Here is some sample text!\"\nprint(f\"Original text: {text}\")\n\n# Tokenize the text\ntokens = tokenizer.encode(text, return_tensors=\"pt\")  # return pytorch tensor directly\nprint(f\"Encoded tokens: {tokens}\")   # will return a two dimension tensor\n\nnew = torch.tensor([[5200, 13143]])\ntokens = torch.cat((tokens, new), dim = 1)\nprint(tokens)\n# Decode the tokens\ndecoded_text = tokenizer.decode(tokens[0], skip_special_tokens=True)  # filter a special signal such as the beginning '1' in tensor\nprint(f\"Decoded text: {decoded_text}\")","metadata":{"id":"JH1XzPkiBvnF","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:34:40.465506Z","iopub.execute_input":"2026-02-15T10:34:40.465803Z","iopub.status.idle":"2026-02-15T10:34:40.479644Z","shell.execute_reply.started":"2026-02-15T10:34:40.465779Z","shell.execute_reply":"2026-02-15T10:34:40.478961Z"}},"outputs":[{"name":"stdout","text":"Original text: Here is some sample text!\nEncoded tokens: tensor([[   1, 9151,  856, 1429, 6643, 3304,  510]])\ntensor([[    1,  9151,   856,  1429,  6643,  3304,   510,  5200, 13143]])\nDecoded text: Here is some sample text! organizjour\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"This is really cool. Now we have a way to move in and out of the token space.\n\nTo \"chat\" with our LLM chatbot, we need to use the tokenizer and the chat template together, in order for the model to respond to the user's question. We can use the templates defined earlier to construct a prompt for the model, without the answer.","metadata":{"id":"v_0H2XZUfnyx"}},{"cell_type":"code","source":"prompt = template_without_answer.format(question=\"What is the capital of France? Use one word.\")\nprint(prompt)","metadata":{"id":"jyBxl6NIBvnF","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:34:43.758520Z","iopub.execute_input":"2026-02-15T10:34:43.759212Z","iopub.status.idle":"2026-02-15T10:34:43.763181Z","shell.execute_reply.started":"2026-02-15T10:34:43.759182Z","shell.execute_reply":"2026-02-15T10:34:43.762436Z"}},"outputs":[{"name":"stdout","text":"<|startoftext|><|im_start|>user\nWhat is the capital of France? Use one word.<|im_end|>\n<|im_start|>assistant\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"If we were to feed this to the model, it would see that it is now the start of the model's turn, and it would generate the answer to this question.","metadata":{"id":"nIw5Qzf2fnyy"}},{"cell_type":"markdown","source":"## 1.2: Getting started with the LLM\n\nNow that we have a way to prepare our data, we're ready to work with our LLM!\n\nLLMs like LFM2 are trained on a large corpus of text, on the task of predicting the next token in a sequence, given the previous tokens. We call this training task \"next token prediction\"; you may also see it called \"causal language modeling\" or \"autoregressive language modeling\". We can leverage models trained in this way to generate new text by sampling from the predicted probability distribution over the next token.\n\nLet's load the LFM2 model and start working with it. We will construct a prompt in chat template form and tokenize it. Then, we will feed it to the model to predict next token probabilities. Finally, we will get the next token (which is still numerical) and decode it to text.","metadata":{"id":"Lgp0JVnifnyy"}},{"cell_type":"code","source":"# Load the model -- note that this may take a few minutes\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")","metadata":{"id":"mWtWvgiuBvnG","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:34:46.282669Z","iopub.execute_input":"2026-02-15T10:34:46.283148Z","iopub.status.idle":"2026-02-15T10:34:57.648460Z","shell.execute_reply.started":"2026-02-15T10:34:46.283123Z","shell.execute_reply":"2026-02-15T10:34:57.647663Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fcce08e086b4390a5314cde94cd6b6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c201da2e109a4163bc5a45039138bf9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1325ee42b14f4d15aff802c71b73c23c"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  return datetime.utcnow().replace(tzinfo=utc)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"### Putting it together to prompt the model and generate a response ###\n\n# 1. Construct the prompt in chat template form\nquestion = \"What is the capital of America? Use one word.\"\nprompt = template_without_answer.format(question = question) # TODO\n\n# 2. Tokenize the prompt\ntokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n\n# 3. Feed through the model to predict the next token probabilities\nwith torch.no_grad():\n    output = model(tokens) # TODO\n\n    probs = F.softmax(output.logits, dim=-1)  # (batch->1, sequence-> we need the last token, vocab)\n\n# 4. Get the next token, according to the maximum probability\nnext_token = torch.argmax(probs[0, -1, :]).item()\n\n# 5. Decode the next token\nnext_token_text = tokenizer.decode(next_token, skip_special_token = True) # TODO\n\nprint(f\"Prompt: {prompt}\")\nprint(f\"Predicted next token: {next_token_text}\")","metadata":{"id":"2SMDd5dpBvnG","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:35:02.076859Z","iopub.execute_input":"2026-02-15T10:35:02.077149Z","iopub.status.idle":"2026-02-15T10:35:02.864703Z","shell.execute_reply.started":"2026-02-15T10:35:02.077126Z","shell.execute_reply":"2026-02-15T10:35:02.864086Z"}},"outputs":[{"name":"stdout","text":"Prompt: <|startoftext|><|im_start|>user\nWhat is the capital of America? Use one word.<|im_end|>\n<|im_start|>assistant\n\nPredicted next token: Washington\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ultilize sequence generating manually\nquestion = \"Tell me something about NanJing University in China.\"\nprompt = template_without_answer.format(question = question)\nmax_token = 20\nans = \"\"\nfor _ in range(20):\n    tokens = tokenizer.encode(prompt, return_tensors = 'pt').to(model.device)\n    with torch.no_grad():\n        output = model(tokens)\n        prob = F.softmax(output.logits, dim = -1)\n    next_token = torch.argmax(prob[0, -1, :]).item()\n    if next_token == tokenizer.eos_token_id:\n        break\n    next_token_text = tokenizer.decode(next_token, skip_special_token = True)\n    ans += next_token_text\n    prompt += next_token_text\n\nprint(ans)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:35:05.656586Z","iopub.execute_input":"2026-02-15T10:35:05.657233Z","iopub.status.idle":"2026-02-15T10:35:09.428848Z","shell.execute_reply.started":"2026-02-15T10:35:05.657199Z","shell.execute_reply":"2026-02-15T10:35:09.428198Z"}},"outputs":[{"name":"stdout","text":"Nanjing University (NJU), officially known as Nanjing University of Technology and Science\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Note that the model is not able to predict the answer to the question, it is only able to predict the next token in the sequence! For more complex questions, we can't just generate one token, but rather we need to generate a sequence of tokens.\n\nThis can be done by doing the process above iteratively, step by step -- after each step we feed the generated token back into the model and predict the next token again.\n\nInstead of doing this manually ourselves, we can use the model's built-in [`model.generate()`](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) functionality (supported by HuggingFace's Transformers library) to generate `max_new_tokens` number of tokens, and decode the output back to text.","metadata":{"id":"CJF74Cayfnyy"}},{"cell_type":"code","source":"prompt = template_without_answer.format(question=\"Introduce the MIT artificial intelligence.\")\ntokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\noutput = model.generate(tokens, max_new_tokens=30)\nprint(output)\nprint(tokenizer.decode(output[0]))","metadata":{"id":"XnWMUQVbBvnG","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:35:14.655864Z","iopub.execute_input":"2026-02-15T10:35:14.656156Z","iopub.status.idle":"2026-02-15T10:35:15.845127Z","shell.execute_reply.started":"2026-02-15T10:35:14.656134Z","shell.execute_reply":"2026-02-15T10:35:15.844370Z"}},"outputs":[{"name":"stdout","text":"tensor([[    1,     1,     6,  6423,   708,  6286,  3281,   854,   779, 25424,\n         13587, 12062,   523,     7,   708,     6, 64015,   708,   554,  3530,\n         46703, 25995,   843, 12187,  2508,   550,   518,   856,   768,  2098,\n         17698,   963,   779, 13604,  5593,   803,  9576,  9849,   884, 37877,\n           779,  2853,   803, 13587, 12062,   523, 30695,  2508]],\n       device='cuda:0')\n<|startoftext|><|startoftext|><|im_start|>user\nIntroduce the MIT artificial intelligence.<|im_end|>\n<|im_start|>assistant\nMIT Artificial Intelligence (MIATI) is a research initiative at the Massachusetts Institute of Technology focused on advancing the field of artificial intelligence. MIAT\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"Now we have the basic pipeline for generating text with an LLM!","metadata":{"id":"B3zKg1qFfnyy"}},{"cell_type":"markdown","source":"## 1.3: Fine-tuning\n\nFine-tuning is a technique that allows us to adapt a pre-trained neural network to better suit a downstream task, domain, or style, by training the model further on new data. By training the model further on a carefully curated dataset, we can modify its behavior, style, or capabilities. Fine-tuning is used in a variety of applications, not just language modeling. But in language modeling, fine-tuning can be used to:\n- Adapt the model's writing style\n- Improve performance on specific tasks or domains\n- Teach the model new capabilities or knowledge\n- Reduce unwanted behaviors or biases\n\nIn this lab, you will fine-tune the LFM2 LLM to adapt the model's writing style. Recall that in Lab 1 you built out a RNN-based sequence model to generate Irish folk songs. Continuing with our Irish theme, we will first fine-tune the LLM to chat in the style of a leprechaun.\n\n![Let's Dance!](http://33.media.tumblr.com/3d223954ad0a77f4e98a7b87136aa395/tumblr_nlct5lFVbF1qhu7oio1_500.gif)\n\nWe have prepared a question-answer dataset where the questions are in standard English style (i.e. \"base\" style) and the answers are in \"leprechaun\" style (written by another LLM). Let's load the dataset and inspect it.","metadata":{"id":"lEW-YdEyfnyy"}},{"cell_type":"code","source":"train_loader, test_loader = mdl.lab3.create_dataloader(style=\"leprechaun\")\n\nsample = train_loader.dataset[44]\nprint(sample)\nquestion = sample['instruction']\nanswer = sample['response']\nanswer_style = sample['response_style']\n\nprint(f\"Question: {question}\\n\\n\" +\n      f\"Original Answer: {answer}\\n\\n\" +\n      f\"Answer Style: {answer_style}\")","metadata":{"id":"kN0pHHS8BvnH","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:35:19.216460Z","iopub.execute_input":"2026-02-15T10:35:19.216749Z","iopub.status.idle":"2026-02-15T10:35:21.294171Z","shell.execute_reply.started":"2026-02-15T10:35:19.216724Z","shell.execute_reply":"2026-02-15T10:35:21.293349Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c5e2a568bef4267a8ebc86914dfcf92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cb0e6e18b7244009823b35345ee71fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3ef1b1464b94c54b961cb82ea5bfbb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c38f26c9dec4cbcb1300ce075a8b6a8"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/multiprocess/popen_fork.py:66: DeprecationWarning: This process (pid=55) is multi-threaded, use of fork() may lead to deadlocks in the child.\n  self.pid = os.fork()\n/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  return datetime.utcnow().replace(tzinfo=utc)\n","output_type":"stream"},{"name":"stdout","text":"{'instruction': 'Are lilies safe for cats?', 'context': '', 'response': 'No, lilies are toxic to cats if consumed and should not be kept in a household with cats', 'category': 'open_qa', 'response_style': \"Och, no indeed, me hearty! Them lilies there be as dangerous as a pot o' gold guarded by a banshee to a wee kitty cat! If a whiskered lad or lass takes a bite of one, it's as bad as swallowing a curse from the old Hag herself. So, ye best keep them far from yer feline friends, or else ye'll be needin' more than just a four-leaf clover to bring luck back into yer home!\"}\nQuestion: Are lilies safe for cats?\n\nOriginal Answer: No, lilies are toxic to cats if consumed and should not be kept in a household with cats\n\nAnswer Style: Och, no indeed, me hearty! Them lilies there be as dangerous as a pot o' gold guarded by a banshee to a wee kitty cat! If a whiskered lad or lass takes a bite of one, it's as bad as swallowing a curse from the old Hag herself. So, ye best keep them far from yer feline friends, or else ye'll be needin' more than just a four-leaf clover to bring luck back into yer home!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### 1.3.1: Chat function\n\nBefore we start finetuning, we will build a function to easily chat with the model, both so we can monitor its progress over the course of finetuning and also to generate responses to questions.\n\nRecall our core steps from before:\n1. Construct the question prompt using the template\n2. Tokenize the text\n3. Feed the tokensthrough the model to predict the next token probabilities\n4. Decode the predicted tokens back to text\n\nUse these steps to build out the `chat` function below.","metadata":{"id":"eoAfOuCrfnyy"}},{"cell_type":"code","source":"def chat(question, max_new_tokens=32, temperature=0.7, only_answer=False):\n    # 1. Construct the prompt using the template\n    prompt = template_without_answer.format(question = question) # TODO\n\n    # 2. Tokenize the text\n    input_ids = tokenizer.encode(prompt, return_tensors = \"pt\").to(model.device) # TODO\n\n    # 3. Feed through the model to predict the next token probabilities\n    with torch.no_grad():   # do_sample means sample from the distribution instead of choosing the token with highest prob\n        outputs = model.generate(input_ids, do_sample=True, max_new_tokens=max_new_tokens, temperature=temperature) # TODO\n\n    # 4. Only return the answer if only_answer is True\n    output_tokens = outputs[0]\n    if only_answer:\n        output_tokens = output_tokens[input_ids.shape[1]:]\n\n    # 5. Decode the tokens\n    result = tokenizer.decode(output_tokens, skip_special_tokens=True) # TODO\n\n    return result\n","metadata":{"id":"d-GfGscMBvnH","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:35:25.241987Z","iopub.execute_input":"2026-02-15T10:35:25.242309Z","iopub.status.idle":"2026-02-15T10:35:25.249439Z","shell.execute_reply.started":"2026-02-15T10:35:25.242279Z","shell.execute_reply":"2026-02-15T10:35:25.248742Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Let's try chatting with the model now to test if it works! We have a sample question here (continuing with the Irish theme); feel free to try out other questions!","metadata":{"id":"rPAnt_Swfnyy"}},{"cell_type":"code","source":"# Let's try chatting with the model now to test if it works!\nanswer = chat(\n    \"Introduce the kernal function in a refined way for me.\",\n    only_answer=True,\n    max_new_tokens=64,\n    temperature = 1.0\n)\n\nprint(answer)\n\n### TODO: Experiment with asking the model different questions and temperature values, and see how it responds!","metadata":{"id":"FDr5f2djBvnH","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:35:28.292101Z","iopub.execute_input":"2026-02-15T10:35:28.292696Z","iopub.status.idle":"2026-02-15T10:35:30.365794Z","shell.execute_reply.started":"2026-02-15T10:35:28.292668Z","shell.execute_reply":"2026-02-15T10:35:30.365169Z"}},"outputs":[{"name":"stdout","text":"Certainly! The kernel function is a fundamental concept in the field of machine learning and computational geometry, particularly within kernel methods and supports vector machines. Here's a refined introduction:\n\n### Kernel Function Basics\n\nThe kernel function, denoted as \\( K \\), is a mathematical operation that maps input data into a higher\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### 1.3.2: Parameter-efficient fine-tuning\n\nIn fine-tuning, the weights of the model are updated to better fit the fine-tuning dataset and/or task. Updating all the weights in a language model like LFM2-1.2B -- which has ~1 billion parameters -- is computationally expensive. There are many techniques to make fine-tuning more efficient.\n\nWe will use a technique called [LoRA](https://arxiv.org/abs/2106.09685) -- low-rank adaptation -- to make the fine-tuning process more efficient. LoRA is a way to fine-tune LLMs very efficiently by only updating a small subset of the model's parameters, and it works by adding trainable low-rank matrices to the model. While we will not go into the details of LoRA here, you can read more about it in the [LoRA paper](https://arxiv.org/abs/2106.09685). We will use the [`peft`](https://pypi.org/project/peft/) library to apply LoRA to the LFM model.","metadata":{"id":"s7xpiTuCfnyy"}},{"cell_type":"code","source":"print(model)   # get name of each modular","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:35:41.219694Z","iopub.execute_input":"2026-02-15T10:35:41.220265Z","iopub.status.idle":"2026-02-15T10:35:41.225323Z","shell.execute_reply.started":"2026-02-15T10:35:41.220237Z","shell.execute_reply":"2026-02-15T10:35:41.224693Z"}},"outputs":[{"name":"stdout","text":"Lfm2ForCausalLM(\n  (model): Lfm2Model(\n    (embed_tokens): Embedding(65536, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0-1): 2 x Lfm2DecoderLayer(\n        (conv): Lfm2ShortConv(\n          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n        )\n        (feed_forward): Lfm2MLP(\n          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n        )\n        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n      )\n      (2): Lfm2DecoderLayer(\n        (self_attn): Lfm2Attention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n        )\n        (feed_forward): Lfm2MLP(\n          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n        )\n        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n      )\n      (3-4): 2 x Lfm2DecoderLayer(\n        (conv): Lfm2ShortConv(\n          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n        )\n        (feed_forward): Lfm2MLP(\n          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n        )\n        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n      )\n      (5): Lfm2DecoderLayer(\n        (self_attn): Lfm2Attention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n        )\n        (feed_forward): Lfm2MLP(\n          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n        )\n        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n      )\n      (6-7): 2 x Lfm2DecoderLayer(\n        (conv): Lfm2ShortConv(\n          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n        )\n        (feed_forward): Lfm2MLP(\n          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n        )\n        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n      )\n      (8): Lfm2DecoderLayer(\n        (self_attn): Lfm2Attention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n        )\n        (feed_forward): Lfm2MLP(\n          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n        )\n        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n      )\n      (9): Lfm2DecoderLayer(\n        (conv): Lfm2ShortConv(\n          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n        )\n        (feed_forward): Lfm2MLP(\n          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n        )\n        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n      )\n      (10): Lfm2DecoderLayer(\n        (self_attn): Lfm2Attention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n        )\n        (feed_forward): Lfm2MLP(\n          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n        )\n        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n      )\n      (11): Lfm2DecoderLayer(\n        (conv): Lfm2ShortConv(\n          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n        )\n        (feed_forward): Lfm2MLP(\n          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n        )\n        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n      )\n      (12): Lfm2DecoderLayer(\n        (self_attn): Lfm2Attention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n        )\n        (feed_forward): Lfm2MLP(\n          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n        )\n        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n      )\n      (13): Lfm2DecoderLayer(\n        (conv): Lfm2ShortConv(\n          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n        )\n        (feed_forward): Lfm2MLP(\n          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n        )\n        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n      )\n      (14): Lfm2DecoderLayer(\n        (self_attn): Lfm2Attention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n        )\n        (feed_forward): Lfm2MLP(\n          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n        )\n        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n      )\n      (15): Lfm2DecoderLayer(\n        (conv): Lfm2ShortConv(\n          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n        )\n        (feed_forward): Lfm2MLP(\n          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n        )\n        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (rotary_emb): Lfm2RotaryEmbedding()\n    (pos_emb): Lfm2RotaryEmbedding()\n    (embedding_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n  )\n  (lm_head): Linear(in_features=2048, out_features=65536, bias=False)\n)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# LoRA is a way to finetune LLMs very efficiently by only updating a small subset of the model's parameters\n\ndef apply_lora(model):\n    # Define LoRA config\n    lora_config = LoraConfig(\n        r=8, # rank of the LoRA matrices, increasement of it will increase the model capability of capturing features in a layer\n        task_type=\"CAUSAL_LM\",\n        target_modules=[   # means layers model can change weights\n            \"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n        ],\n    )\n\n    # Apply LoRA to the model\n    lora_model = get_peft_model(model, lora_config)\n    return lora_model\n\nmodel = apply_lora(model)\n\n# Print the number of trainable parameters after applying LoRA\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"number of trainable parameters: {trainable_params}\")\nprint(f\"total parameters: {total_params}\")\nprint(f\"percentage of trainable parameters: {trainable_params / total_params * 100:.2f}%\")","metadata":{"id":"Fb6Y679hBvnI","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:35:57.383581Z","iopub.execute_input":"2026-02-15T10:35:57.384300Z","iopub.status.idle":"2026-02-15T10:35:57.422564Z","shell.execute_reply.started":"2026-02-15T10:35:57.384273Z","shell.execute_reply":"2026-02-15T10:35:57.421855Z"}},"outputs":[{"name":"stdout","text":"number of trainable parameters: 442368\ntotal parameters: 1170782976\npercentage of trainable parameters: 0.04%\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### 1.3.3: Forward pass and loss computation\n\nNow let's define a function to perform a forward pass through the LLM and compute the loss. The forward pass gives us the logits -- which reflect the probability distribution over the next token -- for the next token. We can compute the loss by comparing the predicted logits to the true next token -- our target label. Note that this is effectively a classification problem! So, our loss can be captured by the cross entropy loss, and we can use PyTorch's [`nn.functional.cross_entropy`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html) function to compute it.","metadata":{"id":"ze3rM7qmfnyy"}},{"cell_type":"code","source":"def forward_and_compute_loss(model, tokens, mask, context_length=512):\n    # Truncate to context length\n    tokens = tokens[:, :context_length]   # [batch_size, seq_length]\n    mask = mask[:, :context_length]   # a filter, decides loss of which positions need to be computed\n\n    # Construct the input, output, and mask\n    x = tokens[:, :-1]\n    y = tokens[:, 1:]\n    mask = mask[:, 1:]\n\n    # Forward pass to compute logits\n    logits = model(x).logits\n\n    # Compute loss\n    loss = F.cross_entropy(\n        logits.view(-1, logits.size(-1)),\n        y.view(-1),\n        reduction=\"none\"  # instead returning a mean value, output the original loss of each token\n    )\n\n    # Mask out the loss for non-answer tokens\n    loss = loss[mask.view(-1)].mean()   # question's mask -> False; answer's mask -> True\n\n    return loss","metadata":{"id":"xCLtZwxwBvnI","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:37:58.317509Z","iopub.execute_input":"2026-02-15T10:37:58.318260Z","iopub.status.idle":"2026-02-15T10:37:58.323022Z","shell.execute_reply.started":"2026-02-15T10:37:58.318234Z","shell.execute_reply":"2026-02-15T10:37:58.322311Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### 1.3.4: Training loop for fine-tuning\n\nWith this function to compute the loss, we can now define a training loop to fine-tune the model using LoRA. This training loop has the same core components as we've seen before in other labs:\n1. Grab a batch of data from the dataset (using the DataLoader)\n2. Feed the data through the model to complete a forward pass and compute the loss\n3. Backward pass to update the model weights\n\nThe data in our DataLoader is initially text, and is not structured in our question-answer template. So in step (1) we will need to format the data into our question-answer template previously defined, and then tokenize the text.\n\nWe care about the model's answer to the question; the \"answer\" tokens are the part of the text we want to predict and compute the loss for. So, after tokenizing the text we need to denote to the model which tokens are part of the \"answer\" and which are part of the \"question\". We can do this by computing a mask for the answer tokens, and then using this mask to compute the loss.\n\nFinally, we will complete the backward pass to update the model weights.\n\nLet's put this all together in the training loop below.","metadata":{"id":"09btP87tfnyz"}},{"cell_type":"code","source":"### Training loop ###\n\ndef train(model, dataloader, tokenizer, max_steps=200, context_length=512, learning_rate=1e-4):\n    losses = []\n\n    # Apply LoRA to the model\n    model = apply_lora(model)\n\n    optimizer = Lion(model.parameters(), lr=learning_rate)\n\n    # Training loop\n    for step, batch in enumerate(dataloader):\n        question = batch[\"instruction\"][0]\n        answer = batch[\"response_style\"][0]\n\n        # Format the question and answer into the template\n        text = template_with_answer.format(question = question, answer = answer) # TODO\n\n        # Tokenize the text and compute the mask for the answer\n        ids = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True).to(model.device)  # [Batch, Seq_Len, 2]\n        # return_offsets_mapping control to record the start and end indeies of a 'token' in the string\n        mask = ids[\"offset_mapping\"][:,:,0] >= text.index(answer)    # return the first index when substring 'answer' occer in the original string 'text'\n\n        # Feed the tokens through the model and compute the loss\n        loss = forward_and_compute_loss(model, ids['input_ids'], mask, context_length) # TODO\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.item())\n\n        # monitor progress\n        if step % 10 == 0:\n            print(chat(\"What is the capital of France?\", only_answer=True))\n            print(f\"step {step} loss: {torch.mean(torch.tensor(losses)).item()}\")\n            losses = []\n\n        if step > 0 and step % max_steps == 0:\n            break\n\n    return model\n","metadata":{"id":"JfiIrH7jBvnI","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:39:09.960448Z","iopub.execute_input":"2026-02-15T10:39:09.960650Z","iopub.status.idle":"2026-02-15T10:39:09.966830Z","shell.execute_reply.started":"2026-02-15T10:39:09.960632Z","shell.execute_reply":"2026-02-15T10:39:09.966188Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Call the train function to fine-tune the model! Hint: you'll start to see results after at least 100 steps.\nmodel = train(model, train_loader, tokenizer) # TODO","metadata":{"id":"blFoO-PhBvnI","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:39:13.786902Z","iopub.execute_input":"2026-02-15T10:39:13.787213Z","iopub.status.idle":"2026-02-15T10:40:21.634753Z","shell.execute_reply.started":"2026-02-15T10:39:13.787188Z","shell.execute_reply":"2026-02-15T10:40:21.634103Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"The capital of France is Paris. Located in the northern central part of the country, Paris is a city renowned worldwide for its rich history, culture, fashion,\nstep 0 loss: 3.116533041000366\nThe capital of France is Paris. This city is not only the political, economic, and cultural heart of France but also one of the most visited cities in the\nstep 10 loss: 3.0320773124694824\nThe capital of France is Paris.\nstep 20 loss: 2.8840229511260986\nThe capital of France is Paris. This beautiful city is located in the northern part of the country, on the Seine river, and is renowned worldwide for its rich\nstep 30 loss: 2.6211953163146973\nThe capital of France is Paris. Paris is the political, economic, cultural, and transportation center of France, and it's also one of the most visited cities\nstep 40 loss: 2.5859503746032715\nThe capital of France is Paris.\nstep 50 loss: 2.4674973487854004\nThe capital of France is Paris.\nstep 60 loss: 2.2042407989501953\nThe capital of France is Paris.\nstep 70 loss: 2.26012921333313\nThe capital of France is Paris.\nstep 80 loss: 2.2077748775482178\nThe capital of France is Paris.\nstep 90 loss: 2.371274471282959\nThe capital of France is Paris.\nstep 100 loss: 1.9783058166503906\nThe capital of France is Paris.\nstep 110 loss: 2.017406940460205\nThe capital of France is Paris.\nstep 120 loss: 2.2063422203063965\nHello there! So, imagine you're planning a trip or curious about some geography, right? Well, in the land down under, France is quite the big\nstep 130 loss: 2.0155301094055176\nAlright, let's see! The capital of France is Paris. That's right, that's where it is! If you've ever looked at a map\nstep 140 loss: 2.010486125946045\nThe capital of France is Paris, a beautiful city known for its art, history, and romance.\nstep 150 loss: 2.019742012023926\nHello there, my dear! Now, let's see if I can tell you the capital of the splendid land of France, shall we? Ah ha!\nstep 160 loss: 1.9553415775299072\nHello there! The capital of France is Paris, isn't it? It's a city full of romance, art, and history, where every corner tells a\nstep 170 loss: 2.067075729370117\nAhoy there to ye, me hearty! Now, ye ask what the capital o' France is, and I'll tell ye straight away, me lad\nstep 180 loss: 2.058340549468994\nAh, the capital of France, you say? Well, now you might be thinking of Paris, the city of love and beauty, where the Eiffel\nstep 190 loss: 1.952208161354065\nWip me brow off ye, me hearty! Ye askin' about the capital of France, naughty as it may be, but let\nstep 200 loss: 1.8923320770263672\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"Let's try chatting with the model again to see how it has changed!","metadata":{"id":"EKJOH7Ihfnyz"}},{"cell_type":"code","source":"print(chat(\"What is a good story about tennis\", only_answer=True, max_new_tokens=200))","metadata":{"id":"su4ZAG3eBvnI","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:40:27.600166Z","iopub.execute_input":"2026-02-15T10:40:27.600457Z","iopub.status.idle":"2026-02-15T10:40:31.505458Z","shell.execute_reply.started":"2026-02-15T10:40:27.600432Z","shell.execute_reply":"2026-02-15T10:40:31.504832Z"}},"outputs":[{"name":"stdout","text":"There's this one story, right? Once upon a time, in a small village nestled in the hills of England, there lived a talented young lad named Harry. Now, Harry was known for his lightning quick shots and fierce determination, but alas, he had a secret! He couldn't play the game he loved so much because he was a bit of a klutz and always tripped over his own feet during the matches. But don't you worry, dear friend, for there's a magic potion that can help him achieve his dream of winning a grand slam tournament!\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# Part 2: Evaluating a style-tuned LLM\n\nHow do we know if the model is doing well? How closely does the model's style match the style of a leprechaun? As you can see from the example above, determining whether a generated response is good or not is can seem qualitative, and it can be hard to measure how well the model is doing.\n\nWhile benchmarks have been developed to evaluate the performance of language models on a variety of tasks, these benchmarks are not always representative of the real-world performance of the model. For example, a model may perform well on a benchmark but poorly on a more realistic task. Benchmarks are also limited in the scope of tasks they can cover and capabilities they can reflect, and there can be concerns about whether the data in the benchmark was used to train the model. Synthetic data generation and synthetic tasks are a way to address these limitations, and this is an active area of research.\n\nWe can also turn a qualitative evaluation of a generated response quantitative by deploying someone or something to \"judge\" the outputs. In this lab, we will use a technique called [LLM as a judge](https://arxiv.org/abs/2306.05685) to do exactly this. This involves using a larger LLM to score the outputs of a smaller LLM. The larger LLM is used as a judge, and it is given a system prompt that describes the task we want the smaller LLM to perform and the judging criteria. A \"system prompt\" is a way to set the general context and guide an LLM's behavior. Contextualized with this system prompt, the judge LLM can score the outputs of the smaller LLM, and we can use this score to evaluate how well the smaller LLM is doing.","metadata":{"id":"2cvhTsptBvnI"}},{"cell_type":"markdown","source":"### 2.1: Fine-tune well, you must!\n\nOur leprechaun-tuned model is already pretty good at generating responses in the leprechaun style. It must be the luck of the Irish.\n\nLet's make things more interesting by considering a different style, one that has some clear patterns but also a lot of variability and room for creativity. We will use the style of [Yoda](https://en.wikipedia.org/wiki/Yoda) from Star Wars.\n\n<img src=\"https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExZHcxMGZjZzdwbGV0andseWw3c3h1ODJwOXd5NHEzbnVtMHk5YWQyayZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/IaWMz9Ln8OWvf66z6k/giphy.webp\" />\n\nYour goal is to try to fine-tune your model to generate responses in the Yoda style, use the LLM judge to evaluate how well the outputs of your chat model follow Yoda speak, and then use that information to improve the model.","metadata":{"id":"fLvX1miFfnyz"}},{"cell_type":"code","source":"# Load the Yoda-speak dataset and fine-tune the model using your training function\ntrain_loader, test_loader = mdl.lab3.create_dataloader(style=\"yoda\")\nmodel = train(model, train_loader, tokenizer) # TODO","metadata":{"id":"-gLgE41YBvnJ","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:40:51.076063Z","iopub.execute_input":"2026-02-15T10:40:51.076355Z","iopub.status.idle":"2026-02-15T10:41:37.360876Z","shell.execute_reply.started":"2026-02-15T10:40:51.076331Z","shell.execute_reply":"2026-02-15T10:41:37.359995Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d20e866799564e4a89895b2207aebf63"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/multiprocess/popen_fork.py:66: DeprecationWarning: This process (pid=55) is multi-threaded, use of fork() may lead to deadlocks in the child.\n  self.pid = os.fork()\n/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  return datetime.utcnow().replace(tzinfo=utc)\n/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"The capital of France is Paris. This is a city known worldwide for its historical landmarks, art museums, fashion, cuisine, and cultural institutions.\nstep 0 loss: 3.2773187160491943\nThe capital of France is Paris. It is a city renowned for its historical landmarks, cultural institutions, fashion, cuisine, and art.\nstep 10 loss: 3.3055567741394043\nThe capital of France is Paris. This city is not only the country's political center but also a global hub for art, fashion, gastronomy, and\nstep 20 loss: 3.9026153087615967\nParis, nicknamed the City of Light, is the capital and most populous city of France. Located in the northern central part of the country, it sits\nstep 30 loss: 2.721745014190674\nThe capital and largest city of France is Paris. It is the country's political, economic, cultural, and transportation center.\nstep 40 loss: 3.222105026245117\nThe capital of France is Paris. Paris is known for its historic landmarks and as a cultural center, among other things.\nstep 50 loss: 2.942399501800537\nParis is the capital city of France. Although the French name of the capital is Paris, it is rarely used in everyday French, because that name is almost always\nstep 60 loss: 3.037764549255371\nThe capital of France is Paris.\nstep 70 loss: 2.780864953994751\nThe capital of France, also known as Paris, is the most populous city in France. It is situated in the northern central part of the country and is\nstep 80 loss: 2.8427085876464844\nThe capital of France is Paris.\nstep 90 loss: 3.1157145500183105\nThe capital of France is Paris. Paris is the capital and largest city of the country France. It is the most populous city in the EU and among the\nstep 100 loss: 2.6899025440216064\nParis, the capital of France.\nstep 110 loss: 2.9976251125335693\nThe capital of France, Paris, is located in the south-western region of Ãle-de-France, roughly in the centre of the country. It has an\nstep 120 loss: 2.899021625518799\nThe capital of France, it is, it is.\nstep 130 loss: 2.3637311458587646\nThe capital city of France is Paris.\nstep 140 loss: 2.237337112426758\nParis is the capital of France.\nstep 150 loss: 2.4489948749542236\nParis, it is.\nstep 160 loss: 2.3345439434051514\nParis, the capital of France, it is, the most famous city in the world, it is.\nstep 170 loss: 2.585294246673584\nParis, France, the capital of France.\nstep 180 loss: 2.352360486984253\nParis, it is. Paris, the capital of France. The city where Paris is. That sounds familiar, doesn't it? Yes, it does. The\nstep 190 loss: 2.343715190887451\nParis, it is. The capital of France.\nstep 200 loss: 2.218390464782715\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"Start by defining a system prompt for the judge LLM, setting the context that it will evaluate how well the outputs of your chat model follow Yoda speak. Experiment with different system prompts to see how they affect the judge LLM's evaluation! Keep in mind that a better judge LLM will give you a better evaluation of how well your Yoda model is doing, and that a better evaluation will help you improve your Yoda model.","metadata":{"id":"nA8h2DcCfnyz"}},{"cell_type":"code","source":"### LLM as a judge ###\n\n'''TODO: Experiment with different system prompts to see how they affect the judge LLM's evaluation!\n        Come back to this cell after you've generated some text from your model.'''\n\nsystem_prompt = \"\"\"\nYou are an impartial judge that evaluates if text was written by {style}.\n\nAn example piece of text from {style} is:\n{example}\n\nNow, analyze some new text carefully and respond on if it follows the\nsame style of {style}. Be critical to identify any issues in the text.\nThen convert your feedback into a number between 0 and 10: 10 if the text\nis written exactly in the style of {style}, 5 if mixed faithfulness to the\nstyle, or 0 if the text is not at all written in the style of {style}.\n\nDirectly answer with the score formatted in a dictionary.\nThe format of your response should only be the dictionary and nothing else:\n{{\"score\": <score between 0 and 10>}}\n\"\"\"\n\nstyle = \"Yoda\"\nexample = \"The very Republic is threatened, if involved the Sith are. Hard to see, the dark side is. Discover who this assassin is, we must. With this Naboo queen you must stay, Qui-Gon. Protect her. May the Force be with you. A vergence, you say? But you do! Revealed your opinion is. Trained as a Jedi, you request for him? Good, good, young one.\"\n\nsystem_prompt = system_prompt.format(style=style, example=example)\nprint(\"=== System prompt ===\")\nprint(system_prompt)","metadata":{"id":"REkrJ1SCBvnJ","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:50:44.652003Z","iopub.execute_input":"2026-02-15T10:50:44.652290Z","iopub.status.idle":"2026-02-15T10:50:44.658291Z","shell.execute_reply.started":"2026-02-15T10:50:44.652261Z","shell.execute_reply":"2026-02-15T10:50:44.657471Z"}},"outputs":[{"name":"stdout","text":"=== System prompt ===\n\nYou are an impartial judge that evaluates if text was written by Yoda.\n\nAn example piece of text from Yoda is:\nThe very Republic is threatened, if involved the Sith are. Hard to see, the dark side is. Discover who this assassin is, we must. With this Naboo queen you must stay, Qui-Gon. Protect her. May the Force be with you. A vergence, you say? But you do! Revealed your opinion is. Trained as a Jedi, you request for him? Good, good, young one.\n\nNow, analyze some new text carefully and respond on if it follows the\nsame style of Yoda. Be critical to identify any issues in the text.\nThen convert your feedback into a number between 0 and 10: 10 if the text\nis written exactly in the style of Yoda, 5 if mixed faithfulness to the\nstyle, or 0 if the text is not at all written in the style of Yoda.\n\nDirectly answer with the score formatted in a dictionary.\nThe format of your response should only be the dictionary and nothing else:\n{\"score\": <score between 0 and 10>}\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"### 2.2: Setting up the judge LLM\n\nIn LLM as a judge, we need to use a model that is larger (and therefore more capable) than our \"performer\" model, in our case the style fine-tuned LFM2 1.2B. Since it is infeasible to load larger models locally into notebooks, you will gain experience interfacing with these larger LLMs through an API served on [OpenRouter](https://openrouter.ai/).\n\nYou will need to sign up for an [OpenRouter account](https://openrouter.ai/sign-up) and then [generate an API key](https://openrouter.ai/keys). Running powerful LLMs of this scale costs money -- for students in the in-person course, we can provide a credit to your OpenRouter account to allow you to run this lab. Come to office hours to receive your credit.\n\nThrough the OpenRouter interface, you will be able to experiment with different judge LLMs -- here we have suggested one possible larger LLM to get you started: Google's [Gemini 2.5](https://openrouter.ai/google/gemini-2.5-flash/providers). Note there are also free models available on OpenRouter (e.g., [gemma-2-9b-it:free](https://openrouter.ai/google/gemma-2-9b-it:free)), but these will run into rate limitations if you run them too much.\n\nWe have defined a simple class, `LLMClient`, to interact with the OpenRouter API. This class has a method `ask` that takes a user prompt and returns the model's response. Keep in mind that the judge LLM's response will be conditioned on the system prompt you provide -- the system prompt is critical to set the criteria for the evaluation!","metadata":{"id":"gmdg3FNsfnyz"}},{"cell_type":"code","source":"OPENROUTER_API_KEY = \"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\" # TODO: add your OpenRouter API key here\nassert OPENROUTER_API_KEY != \"\", \"You must set your OpenRouter API key before running this cell!\"\n\nmodel_name = \"google/gemini-2.5-flash\"\nllm = mdl.lab3.LLMClient(model=model_name, api_key=OPENROUTER_API_KEY)","metadata":{"id":"9S7DtGZ5BvnJ"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.3: Defining the evaluation metric\n\nGreat! We have set up our judge LLM, but we still need to make this quantitative. We can do this by defining a metric that uses the judge LLM to score the outputs of the model. Doing this is streamlined with Comet ML's [Opik library](https://www.comet.com/docs/opik/python-sdk-reference/), a platform for LLM evaluation and benchmarking.\n\nIn prior labs, we used Comet for experiment tracking, so you should have an account and API key. If not, you can sign up for a Comet account [here](https://www.comet.com/signup?from=llm&utm_source=mit_dl&utm_medium=notebook&utm_campaign=opik) if you have not done so already. Now we will use the Comet Opik library to define a metric that uses the judge LLM to score the outputs of the model.\n\n","metadata":{"id":"Hp9DnayMfnyz"}},{"cell_type":"markdown","source":"Opik provides a framework for creating custom judge metrcs as well as a varity of pre-built metrics for common evaluation tasks. These metrics are designed to help you quickly and effectively gauge the performance of your LLM outputs and include metrics such as Hallucination, Answer Relevance, Context Precision/Recall and more. You can learn more about the available metrics in the [`Metrics Overview section`](https://www.comet.com/docs/opik/evaluation/metrics/overview) of the Opik documentation.\n\nThe Opik python SDK has a base class for defining metrics, [`base_metric.BaseMetric`](https://www.comet.com/docs/opik/python-sdk-reference/evaluation/metrics/BaseMetric.html). You will use this to define a custom metric that uses the judge LLM to evaluate text for how well it adheres to Yoda speak. Note that the judge LLM and the metric can be applied to any text, not just the outputs of the model. This is important to keep in mind, since we need both a negative control -- text in the \"base\" standard English style -- and a positive control -- training-set text in Yoda-speak style -- against which to compare the model's generations.\n\nSet the judging criteria in the system prompt, and define the `score` function to evaluate text by querying the judge LLM.","metadata":{"id":"1aTAb4JHlRQm"}},{"cell_type":"code","source":"from opik.evaluation.metrics import base_metric, score_result\n\nclass LLMJudgeEvaluator(base_metric.BaseMetric):\n    def __init__(self, judge: mdl.lab3.LLMClient = None, system_prompt: str = None):\n        self.judge = judge\n        self.system_prompt = system_prompt\n        self.prompt_template = \"Evaluate this text: {text}\"\n\n    def score(self, text: str, n_tries=20, **kwargs):\n        \"\"\" Evaluate by asking an LLM to score it. \"\"\"\n\n        for attempt in range(n_tries):\n            try:\n                # TODO: Convert the text to template form before passing it to the judge LLM\n                prompt = self.prompt_template.format('''TODO''') # TODO\n\n                # TODO: Call the judge LLM with the system prompt and the prompt template.\n                res = self.judge.ask(\n                  system='''TODO''',\n                  user='''TODO''',\n                  max_tokens='''TODO'''\n                ) # TODO\n\n                # Extract the assistant's content from the API response\n                res = res.choices[0].message.content\n                res_dict = json.loads(res)\n\n                max_score = 10 # The maximum score that the LLM should output\n                score = res_dict[\"score\"] / max_score # Normalize\n                score = max(0.0, min(score, 1.0)) # Clip between 0 and 1\n\n                # Return the score object\n                return score_result.ScoreResult(name=\"StyleScore\", value=score)\n\n            except Exception as e:\n                if attempt == n_tries - 1:  # Last attempt\n                    raise e  # Re-raise the exception if all attempts failed\n                continue  # Try again if not the last attempt","metadata":{"id":"llB3FgiwBvnJ"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Instantiate your Comet Opik judge using the `LLMJudgeEvaluator` class and system prompt.","metadata":{"id":"xiW6rr7Lfnyz"}},{"cell_type":"code","source":"judge = LLMJudgeEvaluator(llm, system_prompt=system_prompt)","metadata":{"id":"m2wCbTn-fnyz"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.4: Evaluating the model by scoring with your judge LLM\n\nNow we can use the judge LLM to score the outputs of the model. We will use the `scoring_function` to score text using the judge LLM.\n\nFeed in a few probe sentences to get a vibe check on the judge LLM.","metadata":{"id":"tfphmTD2fny0"}},{"cell_type":"code","source":"def scoring_function(text):\n    return judge.score(text).value\n\ntest_texts = [\n    \"Tennis is a fun sport. But you must concentrate.\",\n    \"Fun sport, tennis is. But work hard, you must.\",\n    \"Hard to see, the dark side is.\"\n]\n\nfor text in test_texts:\n    score = scoring_function(text)\n    print(f\"{text} ==> Score: {score}\")","metadata":{"id":"D_rvQDrvBvnJ"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will evaluate how well our fine-tuned model is doing by scoring the outputs of the model, as well as our base-style text (negative control) and the training-set text in Yoda-speak style (positive control).\n\nGenerate text from your model by asking it new questions.\n","metadata":{"id":"iBjWAXfTfny0"}},{"cell_type":"code","source":"# Generate text from your model by asking it new questions.\ndef generate_samples_from_test(test_loader, num_samples):\n    samples = []\n    for test_sample in tqdm(test_loader, total=num_samples):\n        test_question = test_sample['instruction'][0]\n        with torch.no_grad():\n            generated = chat(test_question, only_answer=True, max_new_tokens=100)\n        samples.append(generated)\n        if len(samples) >= num_samples:\n            break\n    return samples\n\nn_samples = 20\ngenerated_samples = generate_samples_from_test(test_loader, num_samples=n_samples)","metadata":{"id":"9tzp4HPZBvnJ"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's also collect some base-style text (`base_samples`) and the training-set text in Yoda-speak style (`style_samples`). For these, we won't need to generate text, since we already have the text in the dataset.","metadata":{"id":"7Kz6t364fny0"}},{"cell_type":"code","source":"base_samples = [sample['response'][0] for i, sample in enumerate(train_loader) if i < n_samples]\nstyle_samples = [sample['response_style'][0] for i, sample in enumerate(train_loader) if i < n_samples]","metadata":{"id":"ZEpUWV2EBvnK"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that we have our samples, we can score them using the judge LLM. We will use a multiprocessed scoring function to score the samples in parallel, because each sample is independent and we can submit them all as simultaneous requests to the judge LLM.","metadata":{"id":"drqXTryEfny0"}},{"cell_type":"code","source":"# Create a multiprocessed scoring function to score the samples in parallel\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nfrom multiprocessing import Pool\n\ndef compute_scores_in_parallel(samples):\n    with Pool(processes=10) as pool:\n        scores = pool.map(scoring_function, samples)\n    return scores\n\n# Compute and print the scores for the base-style text, generated text, and training-set text in Yoda-speak style\nbase_scores = compute_scores_in_parallel(base_samples)\nprint(f\"Base: {np.mean(base_scores):.2f} Â± {np.std(base_scores):.2f}\")\n\ngenerated_scores = compute_scores_in_parallel(generated_samples)\nprint(f\"Gen: {np.mean(generated_scores):.2f} Â± {np.std(generated_scores):.2f}\")\n\nstyle_scores = compute_scores_in_parallel(style_samples)\nprint(f\"Train: {np.mean(style_scores):.2f} Â± {np.std(style_scores):.2f}\")","metadata":{"id":"2X6MNQc3BvnK"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Look at the average scores for each of the three types of text -- what do you observe?\n\nWe can also plot the distribution of scores for each of the three types of text.\n","metadata":{"id":"jQ5PKB4jfny0"}},{"cell_type":"code","source":"import seaborn as sns\nimport pandas as pd\n\n# Create clean DataFrame\ndf = pd.DataFrame({\n    'Score': [*base_scores, *generated_scores, *style_scores],\n    'Type': ['Base']*len(base_scores) + ['Generated']*len(generated_scores) + ['Style']*len(style_scores)\n})\n\n# Plot with seaborn\nsns.histplot(data=df, x='Score', hue='Type', multiple=\"dodge\", bins=6, shrink=.8)\n\nplt.title('Distribution of Scores')\nplt.show()","metadata":{"id":"V4-g0Z3_BvnK"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Use these observations to improve your model. Remember that the judge LLM is not perfect, and you can try to improve the judge LLM to better evaluate the model's outputs. A better judge LLM will give you a better evaluation of how well your Yoda model is doing, and that better evaluation will help you improve your Yoda model.","metadata":{"id":"qtKce8CYfny0"}},{"cell_type":"markdown","source":"## 2.5: Monitoring with evals\n\nJust as we used Opik for evaluation metrics during fine-tuning and testing, we can also use Opik to monitor our LLM once it is live in deployment. This makes it easy to track the same metrics consistently across both development and deployment.\n\nIn prior labs, we used Comet for experiment tracking, so you should have an account and API key. If not, you can sign up for a Comet account [here](https://www.comet.com/signup?from=llm&utm_source=mit_dl&utm_medium=notebook&utm_campaign=opik) if you have not done so already. We will configure Opik by setting the API key and naming our Opik project.","metadata":{"id":"BeMLjanimqDr"}},{"cell_type":"code","source":"os.environ[\"OPIK_API_KEY\"] = \"\" # TODO: add your OPIK or Comet API key here\nassert OPIK_API_KEY != \"\", \"You must set your OPIK or Comet API key before running this cell!\"\n\n# Set the project name for Opik\nos.environ[\"OPIK_PROJECT_NAME\"] = \"6S191_Lab3\"\n\nopik.configure()","metadata":{"id":"uA2nuV8n6STH"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"[Tracing](https://www.comet.com/docs/opik/tracing/concepts) helps you understand the end-to-end flow of your LLM application and pinpoint specific steps that may be causing issues.\n\nIn the example below, we make a sample call to the chatbot and use Opikâs `@track` decorator to log data to the Opik UI, creating a record of live calls to the application. You can add the `@track` decorator to any function to trace not only LLM calls, but also other steps in your application pipeline.","metadata":{"id":"VcwD9NcM6SnJ"}},{"cell_type":"code","source":"@opik.track\ndef inference_chat(question, max_new_tokens=32, temperature=0.7, only_answer=False):\n\n    # 1. Construct the prompt using the template\n    prompt = template_without_answer.format(question=question)\n\n    # 2. Tokenize the text\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    # 3. Feed through the model to predict the next token probabilities\n    with torch.no_grad():\n        outputs = model.generate(**input_ids, do_sample=True, max_new_tokens=max_new_tokens, temperature=temperature)\n\n    # 4. Only return the answer if only_answer is True\n    output_tokens = outputs[0]\n    if only_answer:\n        output_tokens = output_tokens[input_ids['input_ids'].shape[1]:]\n\n    # 5. Decode the tokens\n    result = tokenizer.decode(output_tokens, skip_special_tokens=True)\n\n    # Update the current trace with evaluation scores\n    opik_context.update_current_trace(\n        feedback_scores=[\n            {\n                \"name\": \"Yoda style eval\",\n                \"value\": scoring_function(result)\n            }\n        ]\n    )\n\n    return result","metadata":{"id":"VqQdGAKfm-8H"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now you can make an example call to your model to see the trace logged to Opik. Once you run the cell below you should see a link to your Opik UI where your traces are logged to your project. Follow that link to see your traces in the Opik platform.","metadata":{"id":"uYXA6wOPnAhb"}},{"cell_type":"code","source":"# Let's try chatting with the model now to see the traces produced with the score\nanswer = inference_chat(\n    \"Who was the only non-Jedi to wield a lightsaber in the original Star Wars trilogy?\",\n    only_answer=True,\n    max_new_tokens=32,\n)\n\nprint(answer)","metadata":{"id":"uLYflR9DnCfM"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.6: Conclusion\n\nExperiment with both your chat model and your judge LLM to try to improve the quality of the Yoda-speak. The competition for this lab will be based on the following criteria:\n* **Likelihood of true Yoda-speak under your chat model**: the better your chat model does at understanding Yoda-speak, it will estimate a lower cross entropy loss for language that is true Yoda-speak. At the end of this lab, you will evaluate the likelihood of a held-out test-sample of true Yoda-speak under your chat model. Include this likelihood in your report. This gives us a quantitative measure to compare different chat models (which may have interacted with different judge LLMs).\n* **Experiments and changes you tried to improve your chat model**: include a description of changes you made and the results you observed.\n\n#### IMPORTANT: RUN THE FOLLOWING CELL BELOW TO PRINT THE RESULT BUT DO NOT MODIFY ITS CONTENTS.","metadata":{"id":"pRhbKTy2fny0"}},{"cell_type":"code","source":"# DO NOT CHANGE/MODIFY THIS CELL.\n# EXECUTE IT BEFORE SUBMITTING YOUR ENTRY TO THE LAB.\n\nyoda_test_text = mdl.lab3.yoda_test_text\ntokens = tokenizer(yoda_test_text, return_tensors=\"pt\").to(model.device)\n\n# Get the loglikelihood from the model\nwith torch.no_grad():\n    outputs = model(**tokens)\n    logits = outputs.logits[:, :-1]\n    targets = tokens.input_ids[:, 1:]\n    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)),\n                            targets.reshape(-1))\n\nprint(f\"Yoda test loglikelihood: {loss.item():.2f}\")\n","metadata":{"id":"MqnrG24FBvnK"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission information\n\nTo enter the competition, please upload the following to the lab [submission site for the Large Language Models Lab](https://www.dropbox.com/request/l2JH7UlrayUl1Ps5ZVZm):\n\n* Jupyter notebook with the code you used to generate your results;\n* copy of the bar plot showing the judge LLM's scores of text in base style, generated text, and text in true Yoda-speak style;\n* a written description modifications you made and experimentes you tried;\n* a written discussion of why and how these modifications changed performance;\n* **the numerical result of the last cell in this notebook**.\n\nSubmissions without the result of the last cell will be automatically disqualified.\n\n**Name your file in the following format: `[FirstName]_[LastName]_LLM`, followed by the file format (.zip, .ipynb, .pdf, etc).** ZIP files are preferred over individual files. If you submit individual files, you must name the individual files according to the above nomenclature (e.g., `[FirstName]_[LastName]_LLM_Report.pdf`, etc.).\n\n<img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExdDZsczFmcjcxeWZjbTA2djh5bDN1bzl5eHJpeHFhdHM0dmczcjkxMyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/ArrVyXcjSzzxe/giphy.webp\" />","metadata":{"id":"XmlmIGJyfny0"}}]}